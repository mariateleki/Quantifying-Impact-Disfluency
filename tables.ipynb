{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f7fceef-a117-4b85-936d-5d79fd3641e1",
   "metadata": {},
   "source": [
    "# Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a9ff21-b7c9-4033-b4d5-ce9fafd55a52",
   "metadata": {},
   "source": [
    "## RQ1 Table: Difference btwn/ N=0 and N=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bec43a8-f60b-4371-8d4a-f3b2dad9c7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import utils_general\n",
    "\n",
    "ROUGE_TYPE = \"rougeL\" # options are: \"rouge1\", \"rouge2\", \"rougeL\", \"rougeLsum\"\n",
    "ROUGE_LIST = [\"rouge1\", \"rouge2\", \"rougeL\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c8bd19-ad25-4221-89e8-46576b8b007a",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 2\n",
    "N_DECIMAL_PLACES = 3\n",
    "\n",
    "table_df = []\n",
    "for m in utils_general.INFERENCE_MODEL_LIST:\n",
    "    \n",
    "    for t in [\"repeats\", \"interjections\", \"false-starts\", \"interjections-and-false-starts\", \"repeats-and-false-starts\", \"repeats-and-interjections\",  \"all-3\"]:\n",
    "\n",
    "        # calculate the mean at 0 for this model/trf\n",
    "        df_zero = pd.read_csv(os.path.join(utils_general.PATH_TO_CSV, \"0\" + \"__\" + m + \"__ROUGE_SCORES.csv\"))\n",
    "        mean_at_zero = df_zero[ROUGE_TYPE].mean()\n",
    "\n",
    "        # calculate the mean at 2 for this model/trf\n",
    "        df_n = pd.read_csv(os.path.join(utils_general.PATH_TO_CSV, t + \"__\" + m + \"__ROUGE_SCORES.csv\"))\n",
    "        df_n = df_n.loc[df_n[\"N_parameter\"] == N]\n",
    "        mean_at_n = df_n[ROUGE_TYPE].mean()\n",
    "\n",
    "        # get the difference\n",
    "        difference_in_mean = mean_at_n - mean_at_zero\n",
    "        mean_at_n = mean_at_n\n",
    "        mean_at_zero = mean_at_zero\n",
    "\n",
    "        table_df.append({\"model\": m,\n",
    "                       \"trf\": t,\n",
    "                       \"mean_at_2\": mean_at_n,\n",
    "                       \"mean_at_0\": mean_at_zero,\n",
    "                       \"difference_in_mean\": difference_in_mean})\n",
    "\n",
    "table_df = pd.DataFrame(table_df)\n",
    "table_df[\"percent_change\"] = (((table_df[\"mean_at_2\"] - table_df[\"mean_at_0\"]) / table_df[\"mean_at_0\"]) * 100)\n",
    "\n",
    "# rounding at the end\n",
    "table_df = table_df.round(N_DECIMAL_PLACES)\n",
    "\n",
    "# display(table_df.pivot(index=[\"model\"], columns=[\"trf\"], values=[\"mean_at_2\",\"mean_at_0\",\"difference_in_mean\",\"percent_change\"]).stack(0))\n",
    "\n",
    "display(table_df.pivot(index=[\"model\"], columns=[\"trf\"], values=[\"mean_at_0\", \"difference_in_mean\", \"percent_change\"]).stack(0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9abd54-47ba-46f0-8794-296583ec1458",
   "metadata": {},
   "source": [
    "## RQ2 Table: Inference Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c0ec62-2f26-4dda-b06b-285187ab0477",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = []\n",
    "\n",
    "for model_name in [\"bart\", \"t5\", \"pegasus\"]:\n",
    "    for test_version in [\"tagged\", \"-1\", \"0\"]:\n",
    "\n",
    "        m = f\"{test_version}__{model_name}__ROUGE_SCORES.csv\"\n",
    "        temp_df = pd.read_csv(os.path.join(utils_general.PATH_TO_CSV,m))\n",
    "\n",
    "        temp_mean_rouge1 = temp_df[\"rouge1\"].mean()\n",
    "        temp_mean_rouge2 = temp_df[\"rouge2\"].mean()\n",
    "        temp_mean_rougeL = temp_df[\"rougeL\"].mean()\n",
    "        temp_mean_rougeLsum = temp_df[\"rougeLsum\"].mean()\n",
    "\n",
    "        df.append({\"model_name\": model_name,\n",
    "                   \"test_version\": test_version,\n",
    "                   \"rouge1\": temp_mean_rouge1,\n",
    "                   \"rouge2\": temp_mean_rouge2,\n",
    "                   \"rougeL\": temp_mean_rougeL,\n",
    "                   \"rougeLsum\": temp_mean_rougeLsum})\n",
    "    \n",
    "df = pd.DataFrame(df)\n",
    "\n",
    "# format the numeric columns\n",
    "numeric_cols = df.select_dtypes(include=np.number).columns\n",
    "df[numeric_cols] = (df[numeric_cols]*100).round(decimals=N_DECIMAL_PLACES)\n",
    "\n",
    "df = df.sort_values(by=[\"model_name\",\"test_version\"])\n",
    "display(df)\n",
    "\n",
    "df.to_csv(os.path.join(utils_general.PATH_TO_CSV, \"RQ2_Table_Inference_Only.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c770632-212b-42d5-bc5c-a315886c3417",
   "metadata": {},
   "source": [
    "## RQ2 Table: Fine-Tuning on Annotated Transcripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efba53d1-abc6-4ae6-8779-11964d87dbc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"TABLE FOR {BART, T5, PEGASUS} FINE-TUNED SCORES\\n\\n\\n\")\n",
    "\n",
    "for model_name in [\"fine-tuned__bart\", \"fine-tuned__pegasus\", \"fine-tuned__t5\"]:\n",
    "    \n",
    "    df = []\n",
    "    \n",
    "    # write rouge score csvs for each of the FINE-TUNED trfs and models\n",
    "    for test_version in [\"tagged\", \"-1\", \"0\"]:\n",
    "        for seed_number in [0, 10, 20, 30, 40]:\n",
    "            for train_version in [\"0\", \"-1\", \"tagged\"]:\n",
    "\n",
    "                train = f\"train_{train_version}\"\n",
    "                test = f\"test_{test_version}\"\n",
    "                csv_name = f\"{model_name}__seed_{seed_number}__{train}__{test}__ROUGE_SCORES.csv\"\n",
    "\n",
    "                temp_df = pd.read_csv(os.path.join(utils_general.PATH_TO_CSV,csv_name))\n",
    "                temp_mean_rouge1 = temp_df[\"rouge1\"].mean()\n",
    "                temp_mean_rouge2 = temp_df[\"rouge2\"].mean()\n",
    "                temp_mean_rougeL = temp_df[\"rougeL\"].mean()\n",
    "                temp_mean_rougeLsum = temp_df[\"rougeLsum\"].mean()\n",
    "\n",
    "                df.append({\"model_name\":model_name,\n",
    "                           \"test_version\": test_version,\n",
    "                           \"train_version\": train_version,\n",
    "                           \"seed_number\": seed_number,\n",
    "                           \"rouge1\": temp_mean_rouge1,\n",
    "                           \"rouge2\": temp_mean_rouge2,\n",
    "                           \"rougeL\": temp_mean_rougeL,\n",
    "                           \"rougeLsum\": temp_mean_rougeLsum})\n",
    "\n",
    "    df = pd.DataFrame(df)\n",
    "    \n",
    "    df = pd.pivot_table(df, values=ROUGE_LIST, index=[\"train_version\", \"test_version\"], aggfunc=\"mean\")\n",
    "    \n",
    "    # format the numeric columns\n",
    "    numeric_cols = df.select_dtypes(include=np.number).columns\n",
    "    df[numeric_cols] = (df[numeric_cols]*100).round(decimals=N_DECIMAL_PLACES)\n",
    "\n",
    "    print(model_name)\n",
    "    display(df)\n",
    "    print(\"\\n\\n\\n\")\n",
    "    df.to_csv(os.path.join(utils_general.PATH_TO_CSV, f\"RQ2_Table_Fine-Tuned_{model_name}.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b414c2d-ce54-422c-9e4e-ff918b29af19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:fig_env]",
   "language": "python",
   "name": "conda-env-fig_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
